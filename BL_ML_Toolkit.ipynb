{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BL_ML_Toolkit.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFhrTJE3/t5u9fzA1JVEKo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Ste2Y_Xb_BHc","executionInfo":{"status":"ok","timestamp":1605622683443,"user_tz":180,"elapsed":607,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["import pandas as pd\n","pd.options.display.float_format = '{:20,.2f}'.format"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"nAD2EO0z_YfF","executionInfo":{"status":"ok","timestamp":1605622685120,"user_tz":180,"elapsed":2275,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","import matplotlib.pyplot as plt\n","\n","class ImputMissingValues(BaseEstimator, TransformerMixin):\n","\n","  drop_list = []\n","\n","  interval_imput_dict = {}\n","  category_imput_dict = {}\n","\n","  def __init__(self, missing_threshold = 0.0, interval_method = 'mean', category_method='mode'):\n","    self.missing_threshold = missing_threshold\n","    self.interval_method = interval_method\n","    self.category_method = category_method\n","  \n","  def fit(self, x, y=None):\n","    #calculate missing fractions\n","    missing_series = x.isnull().sum() / x.shape[0]\n","    self.missing_stats = pd.DataFrame(missing_series).rename(columns = {'index': 'feature', 0: 'missing_fraction'})\n","\n","    #Sorting with highest on top\n","    self.missing_stats = self.missing_stats.sort_values('missing_fraction', ascending = False)\n","    self.record_missing = pd.DataFrame(missing_series[missing_series > self.missing_threshold]).reset_index().rename(columns = {'index': 'feature', 0: 'missing_fraction'})\n","    self.drop_list = list(self.record_missing['feature'])\n","\n","    if self.interval_method == 'mean':\n","      X_data_train_means = x[[k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] != 'category']].mean()\n","      self.interval_imput_dict = X_data_train_means.to_dict()\n","\n","    if self.category_method == 'mode':\n","      X_data_train_modes = x[[k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] == 'category']].mode()\n","      self.category_imput_dict = {k : v[0] for k, v in X_data_train_modes.to_dict().items()}\n","\n","  def transform(self, x):\n","    res = x.drop([f for f in self.drop_list if f in x.columns], axis=1)\n","    res = res.fillna({f:v for f,v in self.interval_imput_dict.items() if f in x.columns})\n","    res = res.fillna({f:v for f,v in self.category_imput_dict.items() if f in x.columns})\n","    return res\n","  \n","  def fit_transform(self, x):\n","    self.fit(x)\n","    return self.transform(x)\n","  \n","  def plot_missing(self):\n","    if self.record_missing is None:\n","      raise NotImplementedError('Missing values have not been calculated. Run `fit` method')\n","\n","    plt.style.use('seaborn-white')\n","    plt.figure(figsize = (7, 5))\n","    plt.hist(self.missing_stats['missing_fraction'], bins = np.linspace(0, 1, 11), edgecolor = 'k', linewidth = 1.5)\n","    plt.xticks(np.linspace(0, 1, 11))\n","    plt.xlabel('Missing Fraction', size = 14)\n","    plt.ylabel('Count of Features', size = 14)\n","    plt.title('Fraction of Missing Values Histogram', size = 16)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJhP5-7hM7_X","executionInfo":{"status":"ok","timestamp":1605622685122,"user_tz":180,"elapsed":2273,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","import numpy as np\n","import itertools\n","\n","class DummyTransformer(BaseEstimator, TransformerMixin):\n","  def __init__(self, nominal_feature_list = None, ignore_feature_list = [], reference_class_choice = 'medium'):\n","    self.ignore_feature_list = ignore_feature_list\n","    self.nominal_feature_list = nominal_feature_list\n","    if reference_class_choice not in ['first', 'last', 'medium']:\n","      raise ValueError('reference_class_choice must be \\'first\\', \\'last\\' or \\'medium\\'')\n","    self.reference_class_choice = reference_class_choice\n","  \n","  def fit(self, x, y=None):\n","    if self.nominal_feature_list is None:\n","      self.nominal_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] == 'category' or k[:5] in ('cbin_', 'ibin_')]\n","    self.nominal_feature_list = [d for d in self.nominal_feature_list if d not in self.ignore_feature_list]\n","    self.dummy_map = {column : {value : int(ix) for ix, value in enumerate(np.sort(x[column].unique()))} for column in self.nominal_feature_list}\n","    self.dummy_inv_map = {feature : {ix : value for value, ix in domain.items()} for feature, domain in self.dummy_map.items()}\n","    if self.reference_class_choice == 'first':\n","      self.dummy_reference_class = {feature : np.min([ix for value, ix in domain.items()]) for feature, domain in self.dummy_map.items() if len(domain) > 1}\n","    elif self.reference_class_choice == 'last':\n","      self.dummy_reference_class = {feature : np.max([ix for value, ix in domain.items()]) for feature, domain in self.dummy_map.items() if len(domain) > 1}\n","    elif self.reference_class_choice == 'medium':\n","      self.dummy_reference_class = {feature : np.sort([ix for value, ix in domain.items()])[(len(domain)-1)//2] for feature, domain in self.dummy_map.items() if len(domain) > 1}\n","    \n","    self.dummy_column_names = {column : [column + '_dummy_' + str(ix) for value, ix in self.dummy_map[column].items()] for column in self.nominal_feature_list}\n","    self.dummy_reference_column = {column : column + '_dummy_' + str(self.dummy_reference_class[column]) for column in self.nominal_feature_list if column in self.dummy_reference_class}\n","    dummy_inv_column_names = [[(dummy, column) for dummy in dummies] for (column, dummies) in self.dummy_column_names.items()]\n","    dummy_inv_column_names = list(itertools.chain.from_iterable(dummy_inv_column_names))\n","    self.dummy_inv_column_names = {k : v for k, v in dummy_inv_column_names}\n","  \n","  def transform(self, x):\n","    res = x\n","    total_obs = res.shape[0]\n","    for column_name in self.nominal_feature_list:\n","      try:\n","        dummy_values = np.zeros((total_obs, len(self.dummy_map[column_name])))\n","      except Exception as ex:\n","        print('Error with column:', column_name)\n","        raise ex\n","      for obs, value in enumerate(x[column_name].array):\n","        if value in self.dummy_map[column_name]:\n","          dummy_values[obs, self.dummy_map[column_name][value]] = 1\n","      \n","      dfDummyValues = pd.DataFrame(dummy_values, columns = self.dummy_column_names[column_name])\n","      dfDummyValues.reset_index(drop=True, inplace=True)\n","      res.reset_index(drop=True, inplace=True)\n","\n","      try:\n","        res = pd.concat([res, dfDummyValues], axis=1)\n","      except Exception as ex:\n","        print('Error with column:', column_name)\n","        raise ex\n","    \n","    res = res.drop(self.nominal_feature_list, axis=1)\n","    res = res.drop([v for k, v in self.dummy_reference_column.items()], axis=1)\n","\n","    return res\n","\n","  def fit_transform(self, x, y=None):\n","    self.fit(x, y)\n","    return self.transform(x)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"8K5Xp0R0UtH4","executionInfo":{"status":"ok","timestamp":1605622685786,"user_tz":180,"elapsed":2933,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","import lightgbm as lgb\n","import gc\n","\n","class FestureImpoertanceVariableSelection(BaseEstimator, TransformerMixin):\n","\n","  def __init__(self, cummulative_importance_threshold, eval_metric='auc', n_folds=2, early_stopping = True, task_type = 'classification'):\n","    self.cummulative_importance_threshold = cummulative_importance_threshold\n","    self.eval_metric = eval_metric\n","    self.n_folds = n_folds\n","    if self.n_folds > 1:\n","      self.skf = StratifiesKFold(n_splits=n_folds)\n","    self.early_stopping = early_stopping\n","    self.task_type = task_type\n","    self.forced_drop_list = []\n","    self.forced_select_list = []\n","  \n","  def set_manual_select_drop_list(self, forced_drop_list = [], forced_select_list = []):\n","    for v in forced_select_list:\n","      if v in forced_drop_list:\n","        raise ValueError('A variable cannot be in select and drop list at the same time')\n","    \n","    self.forced_drop_list += forced_drop_list\n","    self.forced_select_list += forced_select_list\n","  \n","  def fit(self, x, y):\n","    self.nominal_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] == 'category']\n","    self.interval_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] != 'category']\n","\n","    dt = DummyTransformer(nominal_feature_list=self.nominal_feature_list)\n","\n","    #One hot encoding\n","    features = dt.fit_transform(x)\n","    self.dt = dt\n","    self.one_hot_features = [column for column in features.columns if column not in self.nominal_feature_list + self.interval_feature_list]\n","\n","    #Extract feature names\n","    feature_names = list(features.columns)\n","\n","    #Convert to np array\n","    features = np.array(features)\n","    labels = np.array(y)\n","\n","    #Empty array for feature importances\n","    feature_importance_values = np.zeros(len(feature_names))\n","\n","    print('Training Gradient Boosting Model\\n')\n","\n","    if self.n_folds == 1 or not self.early_stopping:\n","\n","      if self.task_type == 'classification':\n","        model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","      elif self.task_type == 'regression':\n","        model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","      else:\n","        raise ValueError('Task must be either \"classification\" or \"regression\"')\n","      \n","      #Train the model\n","      model.fit(features, labels)\n","\n","      #Record the feature importances\n","      feature_importance_values += model.feature_importances_\n","    \n","    else:\n","\n","      if self.task_type == 'classification':\n","        model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","      elif self.task_type == 'regression':\n","        model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","      else:\n","        raise ValueError('Task must be either \"classification\" or \"regression\"')\n","      \n","      #Iterate through each fold\n","      for train_index, valid_index in self.skf.split(features, labels):\n","        train_features = features[train_index]\n","        valid_features = features[valid_index]\n","        train_labels = labels[train_index]\n","        valid_labels = labels[valid_index]\n","\n","        #Train the model with early stopping\n","        model.fit(train_features, train_labels, eval_metric = self.eval_metric, eval_set = [(valid_features, valid_labels)], early_stopping_rounds = 100, verbose = -1)\n","\n","        #Clean up memory\n","        gc.enable()\n","        del train_features, train_labels, valid_features, valid_labels\n","        gc.collect()\n","\n","        #Record the feature importances\n","        feature_importance_values += model.feature_importance_\n","    \n","    feature_importance_values /= self.n_folds\n","\n","    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n","\n","    #Add the base\n","    feature_importances['base_feature'] = feature_importances['feature'].apply(lambda x: dt.dummy_inv_column_names[x] if x in dt.dummy_inv_column_names else x)\n","\n","    #Sort features according to importance\n","    feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n","\n","    #Aggregate features by its base form\n","    feature_importances_grouped = feature_importances.groupby('base_feature')['importance'].sum().to_frame().reset_index().sort_values('importance', ascending = False)\n","\n","    #Normalize the feature importances to add up to one\n","    feature_importances_grouped['normalized_importance'] = feature_importances_grouped['importance'] / feature_importances_grouped['importance'].sum()\n","    feature_importances_grouped['cumulative_importance'] = np.cumsum(feature_importances_grouped['normalized_importance'])\n","    self.feature_importances = feature_importances_grouped\n","\n","    #Extract the features with zero importance\n","    self.zero_importance_features = self.feature_importances[self.feature_importances['importance'] == 0.0]\n","    self.drop_list_zero_importance = list(self.zero_importance_features['base_feature'])\n","\n","    #Identify the features not needed to reach the cummulative importance\n","    record_low_importance = self.feature_importances[self.feature_importances['cumulative_importance'] > self.cummulative_importance_threshold]\n","    self.drop_list_low_importance = list(record_low_importance['base_feature'])\n","  \n","  def transform(self, x):\n","    drop_list = list(np.unique(self.drop_list_zero_importance + self.drop_list_low_importance + self.forced_drop_list))\n","    drop_list = [d for d in drop_list if d not in self.forced_select_list]\n","\n","    res = x.drop(drop_list, axis=1)\n","    return res\n","  \n","  def fit_transform(self, x, y=None):\n","    self.fit(x, y)\n","    return self.transform(x)\n","  \n","  def plot_feature_importances(self, plot_n = 15, threshold = None):\n","    if self.feature_importances is None:\n","      raise NotImplementedError('Feature importances have not been determined. Run `fit`')\n","  \n","    #Need to adjust number of features if greater than the features in the data\n","    if plot_n > self.feature_importances.shape[0]:\n","      plot_n = self.feature_importances.shape[0] - 1\n","    \n","    #Make a horizontal bar chart of feature importances\n","    plt.figure(figsize = (8, 6))\n","    ax = plt.subplot()\n","\n","    #Need to reverse the index to plot most important on top\n","    ax.barh(np.arange(plot_n),\n","            self.feature_importances['normalized_feature'][:plot_n],\n","            align = 'center', edgecolor = 'k')\n","    \n","    #Set the yticks and labels\n","    ax.set_yticks(np.arange(plot_n))\n","    ax.set_yticklabels(self.feature_importances['base_feature'][:plot_n], size = 10)\n","\n","    plt.gca().invert_yaxis()\n","\n","    #Plot labeling\n","    plt.xlabel('Normalized Importance', size = 16)\n","    plt.title('Feature Importances', size = 18)\n","    plt.show()\n","\n","    #Cumulative importance plot\n","    plt.figure(figsize = (8, 6))\n","    plt.plot(list(range(1, len(self.feature_importances) + 1)), self.feature_importances['cumulative_importance'], 'r-')\n","    plt.xlabel('Number of Features', size = 14)\n","    plt.ylabel('Cumulative Importance', size = 14)\n","    plt.title('Cumulative Feature Importance', size = 16)\n","\n","    if threshold:\n","      #Index of minimum number of features needed for cumulative importance threshold\n","      #np.where returns the index so need to add 1 to have correct number\n","      importance_index = np.min(np.where(self.feature_importances['cumulative_importance'] > threshold))\n","      plt.vlines(x = importance_index + 1, ymin = 0, ymax = 1, linestyles = '--', colors = 'blue')\n","      plt.show()\n","\n","      print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"26BRMP6DyiSZ","executionInfo":{"status":"ok","timestamp":1605622686628,"user_tz":180,"elapsed":3771,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression, Lasso\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.preprocessing import StandardScaler\n","import gc\n","\n","class RegularizationFeatureSelection(BaseEstimator, TransformerMixin):\n","\n","  def __init__(self, n_folds=2, RegularizationStrength = 1.0, task_type = 'classification'):\n","    self.n_folds = n_folds\n","    self.RegularizationStrength = RegularizationStrength\n","    if self.n_folds > 1:\n","      self.skf = StratifiesKFold(n_splits=n_folds)\n","    self.task_type = task_type\n","    self.forced_drop_list = []\n","    self.forced_select_list = []\n","  \n","  def set_manual_select_drop_list(self, forced_drop_list = [], forced_select_list = []):\n","    for v in forced_select_list:\n","      if v in forced_drop_list:\n","        raise ValueError('A variable cannot be in select and drop list at the same time')\n","    \n","    self.forced_drop_list += forced_drop_list\n","    self.forced_select_list += forced_select_list\n","  \n","  def fit(self, x, y):\n","    self.nominal_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] == 'category']\n","    self.interval_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] != 'category']\n","\n","    dt = DummyTransformer(nominal_feature_list=self.nominal_feature_list)\n","\n","    #One hot encoding\n","    features = dt.fit_transform(x)\n","    self.dt = dt\n","    self.one_hot_features = [column for column in features.columns if column not in self.nominal_feature_list + self.interval_feature_list]\n","\n","    fold_selected_features = np.zeros((self.n_folds, features.shape[1]), dtype=bool)\n","    fold_selected_features_coefs = np.zeros((self.n_folds, features.shape[1]), dtype=float)\n","\n","    #Extract feature names\n","    feature_names = features.columns\n","\n","    #Convert to np array\n","    features = np.array(features)\n","    labels = np.array(y)\n","\n","    print('Training Lasso Regression Model\\n')\n","\n","    if self.n_folds > 1:\n","      #Iterate through each fold\n","      for train_index, valid_index in self.skf.split(features, labels):\n","        scaler = StandardScaler()\n","        if self.task_type == 'classification':\n","          model = SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear', C=self.RegularizationStrength, class_weight = 'balanced'))\n","        elif self.task_type == 'regression':\n","          model = SelectFromModel(Lasso(alpha=self.RegularizationStrength))\n","        else:\n","          raise ValueError('Task type must be either \"classification\" or \"regression\"')\n","\n","        train_features = features[train_index]\n","        train_labels = labels[train_index]\n","\n","        #Train the model\n","        scaled_features = scaler.fit_transform(train_features)\n","        model.fit(scaled_features, train_labels)\n","    \n","        #Record the feature importances\n","        fold_selected_features[self.n_folds-1,:] = model.get_support()\n","        fold_selected_features_coefs[self.n_folds-1,:] = model.estimator_.coef_\n","\n","        #Clean up memory\n","        gc.enable()\n","        del train_features, train_labels\n","        gc.collect()\n","\n","    else:\n","      scaler = StandardScaler()\n","      if self.task_type == 'classification':\n","        model = SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear', C=self.RegularizationStrength, class_weight = 'balanced'))\n","      elif self.task_type == 'regression':\n","        model = SelectFromModel(Lasso(alpha=self.RegularizationStrength))\n","      else:\n","        raise ValueError('Task type must be either \"classification\" or \"regression\"')\n","      scaled_features = scaler.fit_transform(features)\n","      model.fit(scaled_features, y)\n","\n","      #Record the feature importances\n","      fold_selected_features[self.n_folds-1,:] = model.get_support()\n","      fold_selected_features_coefs[self.n_folds-1,:] = model.estimator_.coef_\n","  \n","    selected_features = np.sum(fold_selected_features, axis = 0, dtype = bool)\n","    selected_features_coefs = np.mean(fold_selected_features_coefs, axis = 0, dtype = float)\n","\n","    self.selected_features = feature_names[selected_features]\n","    self.selected_features_averaged_coefs = selected_features_coefs[selected_features]\n","    list_zero_coeficient = feature_names[selected_features == False]\n","\n","    interval_list_zero_coeficient = list(np.array([v for v in list_zero_coeficient if v not in dt.dummy_inv_column_names]))\n","    nominal_list_zero_coeficient = list(np.unique([dt.dummy_inv_column_names[v] for v in list_zero_coeficient if v in dt.dummy_inv_column_names]))\n","    nominal_list_nonzero_coeficient = list(np.unique([dt.dummy_inv_column_names[v] for v in self.selected_features if v in dt.dummy_inv_column_names]))\n","    \n","    self.drop_list_zero_coeficient = interval_list_zero_coeficient + [v for v in nominal_list_zero_coeficient if v not in nominal_list_nonzero_coeficient]\n","\n","    dfNonZeroAbsoluteCoefs = pd.DataFrame(list(zip(self.selected_features, self.selected_features_averaged_coefs, np.abs(self.selected_features_averaged_coefs))), columns=['feature', 'coef', 'absolute_coef'])\n","    dfZeroAbsoluteCoefs = pd.DataFrame(list(zip(self.drop_list_zero_coeficient, [0.0] * len(self.drop_list_zero_coeficient), [0.0] * len(self.drop_list_zero_coeficient))), columns=['feature', 'coef', 'absolute_coef'])\n","    dfAbsoluteCoefs = dfNonZeroAbsoluteCoefs.append(dfZeroAbsoluteCoefs, ignore_index=True)\n","    self.dfAbsoluteCoefs = dfAbsoluteCoefs.sort_values('absolute_coef', ascending = False)\n","\n","  def transform(self, x):\n","    drop_list = list(np.unique(self.drop_list_zero_coeficient + self.forced_drop_list))\n","    drop_list = [d for d in drop_list if d not in self.forced_select_list]\n","\n","    res = x.drop(drop_list, axis=1)\n","    return res\n","  \n","  def fit_transform(self, x, y=None):\n","    self.fit(x, y)\n","    return self.transform(x)\n","  \n","  def plot_feature_coefs(self, plot_n = 15, threshold = None):\n","    if self.dfAbsoluteCoefs is None:\n","      raise NotImplementedError('Feature coefs have not been determined. Run `fit`')\n","  \n","    #Need to adjust number of features if greater than the features in the data\n","    if plot_n > self.dfAbsoluteCoefs.shape[0]:\n","      plot_n = self.dfAbsoluteCoefs.shape[0] - 1\n","\n","    colors = ['b' if c > 0.0 else 'r' for c in list(self.dfAbsoluteCoefs['coef'])]\n","    \n","    #Make a horizontal bar chart of feature importances\n","    plt.figure(figsize = (10, 6))\n","    ax = plt.subplot()\n","\n","    #Need to reverse the index to plot most important on top\n","    ax.barh(np.arange(plot_n),\n","            self.dfAbsoluteCoefs['coef'][:plot_n],\n","            align = 'center', edgecolor = 'k', color=colors)\n","\n","    plt.gca().invert_yaxis()\n","\n","    #Set the yticks and labels\n","    ax.set_yticks(np.arange(plot_n))\n","    ax.set_yticklabels(self.dfAbsoluteCoefs['feature'][:plot_n], size = 12)\n","\n","    #Plot labeling\n","    plt.xlabel('Coeficient', size = 16)\n","    plt.title('Feature Coeficients', size = 18)\n","    plt.show()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uPK1uyan8La","executionInfo":{"status":"ok","timestamp":1605622686858,"user_tz":180,"elapsed":3997,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.cluster import AgglomerativeClustering\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","class VariableClusteringSelection(BaseEstimator, TransformerMixin):\n","\n","  def __init__(self, n_folds=2, dissimilarity_threshold = 0.5, use_class_features = False):\n","    self.dissimilarity_threshold = dissimilarity_threshold\n","    self.use_class_features = use_class_features\n","    self.forced_drop_list = []\n","    self.forced_select_list = []\n","\n","  def set_manual_select_drop_list(self, forced_drop_list = [], forced_select_list=[]):\n","    for v in forced_select_list:\n","      if v in forced_drop_list:\n","        raise ValueError('A variable cannot be in select and drop list at the same time')\n","    \n","    self.forced_drop_list += forced_drop_list\n","    self.forced_select_list += forced_select_list\n","  \n","  def fit(self, x, y=None):\n","    self.nominal_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] == 'category']\n","    self.interval_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] != 'category']\n","\n","    dt = DummyTransformer(nominal_feature_list=self.nominal_feature_list)\n","\n","    if self.use_class_features:\n","      #One hot encoding\n","      features = dt.fit_transform(x)\n","      self.dt = dt\n","      self.one_hot_features = [column for column in features.columns if column not in self.nominal_feature_list + self.interval_feature_list]\n","    else:\n","      features = x[self.interval_feature_list]\n","      self.set_manual_select_drop_list(forced_select_list = self.nominal_feature_list)\n","    \n","    #correlation matrix\n","    corr_matrix = features.corr()\n","\n","    #dissimilarity matrix\n","    dissimilarity_matrix = 1 - np.abs(corr_matrix)\n","\n","    #variable clustering\n","    print('Fitting Agglomerative Clustering\\n')\n","    clustering = AgglomerativeClustering(linkage='ward', n_clusters=None, distance_threshold = self.dissimilarity_threshold)\n","    clustering.fit(dissimilarity_matrix)\n","\n","    #cluster proximities\n","    clusters_proximities = []\n","    for cluster_row in np.sort(np.unique(clustering.labels_)):\n","      for cluster_column in range(cluster_row + 1, np.max(clustering.labels_) + 1):\n","        clusters_proximities.append([cluster_row, cluster_column,\n","                                     np.mean(dissimilarity_matrix.loc[dissimilarity_matrix.columns[np.where(clustering.labels_ == cluster_row)[0]]].values)])\n","        clusters_proximities.append([cluster_column, cluster_row,\n","                                     np.mean(dissimilarity_matrix.loc[dissimilarity_matrix.columns[np.where(clustering.labels_ == cluster_row)[0]]].values)])\n","    \n","    df_clusters_proximities = pd.DataFrame(clusters_proximities, columns=['cluster', 'neighbor', 'dissimilarity'])\n","\n","    #nearest cluster\n","    df_nearest_cluster = df_clusters_proximities.sort_values('dissimilarity').groupby('cluster').head(1)\n","\n","    #variable scores\n","    variable_scores = []\n","\n","    for ix, row in df_nearest_cluster.iterrows(): #foreach cluster\n","      cluster_variables = np.where(clustering.labels_ == row['cluster'])[0] #own clusters variables\n","      neighbor_variables = np.where(clustering.labels_ == row['neighbor'])[0] #nearest clusters variables\n","\n","      #if one-variable cluster\n","      if len(cluster_variables) == 1:\n","        Y_corr = features[dissimilarity_matrix.columns[cluster_variables[0]]]\n","        X_corr = features[dissimilarity_matrix.columns[neighbor_variables]]\n","        reg = LinearRegression().fit(X_corr, Y_corr)\n","        neighbor_score = reg.score(X_corr, Y_corr)\n","\n","        variable_scores.append([row['cluster'], dissimilarity_matrix.columns[cluster_variables[0]], 1.0, neighbor_score, (1 - 1)/(1 - neighbor_score)])\n","      else:\n","        for v in cluster_variables:\n","          Y_corr = features[dissimilarity_matrix.columns[v]]\n","          X_corr = features[dissimilarity_matrix.columns[cluster_variables[np.where(np.array(cluster_variables) != v)]]]\n","          reg = LinearRegression().fit(X_corr, Y_corr)\n","          cluster_score = reg.score(X_corr, Y_corr)\n","\n","          Y_corr = features[dissimilarity_matrix.columns[v]]\n","          X_corr = features[dissimilarity_matrix.columns[neighbor_variables]]\n","          reg = LinearRegression().fit(X_corr, Y_corr)\n","          neighbor_score = reg.score(X_corr, Y_corr)\n","\n","          variable_scores.append([row['cluster'], dissimilarity_matrix.columns[v], cluster_score, neighbor_score, (1 - cluster_score)/(1 - neighbor_score)])\n","    \n","    df_variable_scores = pd.DataFrame(variable_scores, columns=['cluster', 'variable', 'own_cluster_Rˆ2', 'neighbor_cluster_Rˆ2', '1-Rˆ2 Ratio'])\n","\n","    df_best_variables = df_variable_scores.sort_values('1-Rˆ2 Ratio').groupby('cluster').head(1)\n","\n","    #results\n","    self.corr_matrix = corr_matrix\n","    self.clustering = clustering\n","    self.df_clusters_proximities = df_clusters_proximities\n","    self.df_nearest_cluster = df_nearest_cluster\n","    self.df_variable_scores = df_variable_scores\n","    self.df_best_variables = df_best_variables\n","    self.select_list_best_variables = list(df_best_variables['variable'].values)\n","  \n","  def transform(self, x):\n","    if self.use_class_features:\n","      #One hot encoding\n","      features = self.dt.transform(x)\n","    else:\n","      features = x\n","    \n","    res = features[np.unique(self.select_list_best_variables + self.forced_select_list)]\n","    res = res.drop(self.forced_drop_list, axis=1)\n","    return res\n","  \n","  def fit_transform(self, x, y=None):\n","    self.fit(x, y)\n","    return self.transform(x)\n","  \n","  def plot_collinear(self, correlation_threshold = 0.7, cmap='bwr'):\n","    \"\"\"\n","    Adapted from reference: https://github.com/WillKoehrsen/feature-selector/blob/master/feature_selector/feature_selector.py\n","    \"\"\"\n","    upper = self.corr_matrix.where(np.triu(np.ones(self.corr_matrix.shape), k = 1).astype(np.bool))\n","\n","    #Dataframe to hold correlated pairs\n","    record_collinear = pd.DataFrame(columns = ['feature', 'corr_feature', 'corr_value'])\n","\n","    for c in upper.columns:\n","      #Find the correlated features\n","      corr_features = list(upper.index[upper[c].abs() > correlation_threshold])\n","      if len(corr_features) == 0:\n","        continue\n","      #Find the correlated features\n","      corr_values = list(upper[c][upper[c].abs() > correlation_threshold])\n","      features = [c for _ in range(len(corr_features))]\n","\n","      #Record the information (need a temp df for now)\n","      temp_df = pd.DataFrame.from_dict({'feature': features,\n","                                        'corr_feature': corr_features,\n","                                        'corr_value': corr_values})\n","      \n","      #Add to dataframe\n","      record_collinear = record_collinear.append(temp_df, ignore_index = True)\n","    \n","    record_collinear.sort_values(['feature', 'corr_value'], ascending=[True, False])\n","\n","    #Identify the correlations that were above the threshold\n","    #columns (x-axis) are features to drop and rows (y-axis) are correlated pairs\n","    corr_matrix_plot = self.corr_matrix.loc[list(set(record_collinear['corr_feature'])),\n","                                            list(set(record_collinear['feature']))]\n","    \n","    if len(corr_matrix_plot) == 0:\n","      print('No correlation above threshold')\n","      return\n","    \n","    corr_matrix_plot = corr_matrix_plot.sort_values(list(corr_matrix_plot.columns)).sort_values(list(corr_matrix_plot.index), axis=1)\n","    self.corr_matrix_plot = corr_matrix_plot\n","\n","    title = \"Reduced Correlation Above Threshold Matrix\"\n","\n","    f, ax = plt.subplots(figsize=(10, 8))\n","\n","    #Draw the heatmap with a color bar\n","    sns.heatmap(corr_matrix_plot, center=0, cmap=cmap,\n","                linewidths=.25, cbar_kws={\"shrink\": 0.6})\n","    \n","    #Set the ylabels\n","    ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])\n","    ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]))\n","\n","    #Set the xlabels\n","    ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])\n","    ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]))\n","\n","    plt.title(title, size=14)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"v6hVw76bBjJS","executionInfo":{"status":"ok","timestamp":1605622688154,"user_tz":180,"elapsed":5290,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy\n","\n","class OptimalNominalBinning(BaseEstimator, TransformerMixin):\n","\n","  def __init__(self, sigLevel, rare_cutoff_percentage=0.005, balanced=False, verbose= 0, ignore_feature_list = [], max_bins = 10, task_type = 'classification'):\n","    self.sigLevel = sigLevel\n","    self.rare_cutoff_percentage = rare_cutoff_percentage\n","    self.balanced = balanced\n","    self.verbose = verbose\n","    self.ignore_feature_list = ignore_feature_list\n","    self.task_type = task_type\n","    self.max_bins = max_bins\n","    self.map = {}\n","    self.original_map = {}\n","    self.inv_map = {}\n","    self.original_inv_map = {}\n","    self.statistics = {}\n","    self.original_statistics = {}\n","  \n","  def fit(self, x, y):\n","    self.x = x\n","    self.y = y\n","    self.targetColumn = y.columns[0]\n","    series = [x, y[self.targetColumn].reset_index(drop = True)]\n","    self.AllData = pd.concat(series, axis=1)\n","\n","    self.nominal_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] == 'category' or '_dummy_' in k or k[:5] in ('cbin_', 'ibin_')]\n","    self.nominal_feature_list = [k for k in self.nominal_feature_list if k not in self.ignore_feature_list]\n","    self.interval_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] != 'category' and '_dummy_'not in k and k[:5] not in ('cbin_', 'ibin_')]\n","\n","    for feature in x[self.nominal_feature_list].columns:\n","      dfBinFeature = self.optimalClassGrouping(x[self.nominal_feature_list], y, feature)\n","      self.map[feature] = {k : v for k, v in dfBinFeature.drop_duplicates([feature, 'bin'])[[feature, 'bin']].values}\n","      self.original_map[feature] = self.map[feature].copy()\n","\n","      self.inv_map[feature] = {}\n","      for v, b in self.map[feature].items():\n","        if b not in self.inv_map[feature]:\n","          self.inv_map[feature][b] = [v]\n","        else:\n","          self.inv_map[feature][b].append(v)\n","      \n","      self.original_inv_map[feature] = self.inv_map[feature].copy()\n","\n","      self.statistics[feature] = pd.DataFrame([k for k, v in self.inv_map[feature].items()], columns=['bin']).sort_values('bin').reset_index(drop=True)\n","\n","      def bin_count(x):\n","        b = x\n","        return self.AllData[self.AllData[feature].isin(self.inv_map[feature][b])][self.targetColumn].count()\n","\n","      def bin_mean(x):\n","        b = x\n","        return self.AllData[self.AllData[feature].isin(self.inv_map[feature][b])][self.targetColumn].mean()\n","\n","      def bin_sum(x):\n","        b = x\n","        return self.AllData[self.AllData[feature].isin(self.inv_map[feature][b])][self.targetColumn].sum()\n","      \n","      def get_class_list(x):\n","        if x not in self.inv_map[feature]:\n","          raise ValueError('bin ' + str(x) + ' does not exist for the feature ' + str(feature))\n","        return str(self.inv_map[feature][x])\n","\n","      self.statistics[feature]['n_obs'] = self.statistics[feature]['bin'].apply(bin_count)\n","      self.statistics[feature]['n_events'] = self.statistics[feature]['bin'].apply(bin_sum)\n","      self.statistics[feature]['event_rate'] = self.statistics[feature]['bin'].apply(bin_mean)\n","      self.statistics[feature]['class_list'] = self.statistics[feature]['bin'].apply(get_class_list)\n","\n","      self.original_statistics[feature] = self.statistics[feature].copy()\n","\n","  def transform(self, x):\n","    features = x\n","\n","    for feature in self.x[self.nominal_feature_list].columns:\n","      if feature in self.ignore_feature_list:\n","        continue\n","      \n","      for v in np.unique(self.x[feature].values):\n","        if v not in self.map[feature]:\n","          print('column', feature, 'Value:', v)\n","          raise NotImplementedError('Valur out of the bounds')\n","      features['cbin_' + feature] = features[feature].apply(lambda x: self.map[feature][x] if x in self.map[feature] else None)\n","      features = features.drop([feature], axis=1)\n","    \n","    return features\n","  \n","  def ignore_feature(self, feature):\n","    self.ignore_feature_list.append(feature)\n","  \n","  def reset_feature(self, feature):\n","    if feature not in self.x[self.nominal_feature_list].columns:\n","      raise NotImplementedError('Referenced feature does not exists in trained DataFrame')\n","    self.map[feature] = self.original_map[feature]\n","    self.inv_map[feature] = self.original_inv_map[feature]\n","    self.statistics[feature] = self.original_statistics[feature]\n","  \n","  def changeFeatureBin(self, feature, feature_value, bin_value):\n","    if feature not in self.x[self.nominal_feature_list].columns:\n","      raise NotImplementedError('Referenced feature does not exists in trained DataFrame')\n","    if feature not in self.map[feature]:\n","      raise NotImplementedError('Referenced value does not exists in feature domain')\n","    self.map[feature][feature_value] = bin_value\n","\n","    self.inv_map[feature] = {}\n","    for v, b in self.map[feature].items():\n","      if b not in self.inv_map[feature]:\n","        self.inv_map[feature][b] = [v]\n","      else:\n","        self.inv_map[feature][b].append(v)\n","    \n","    self.original_inv_map[feature] = self.inv_map[feature].copy()\n","\n","    self.statistics[feature] = pd.DataFrame([k for k, v in self.inv_map[feature].items()], columns=['bin']).sort_values('bin').reset_index(drop=True)\n","\n","    def bin_count(x):\n","      b = x\n","      return self.AllData[self.AllData[feature].isin(self.inv_map[feature][b])][self.targetColumn].count()\n","\n","    def bin_mean(x):\n","      b = x\n","      return self.AllData[self.AllData[feature].isin(self.inv_map[feature][b])][self.targetColumn].mean()\n","\n","    def bin_sum(x):\n","      b = x\n","      return self.AllData[self.AllData[feature].isin(self.inv_map[feature][b])][self.targetColumn].sum()\n","    \n","    def get_class_list(x):\n","      if x not in self.inv_map[feature]:\n","        raise ValueError('bin ' + str(x) + ' does not exist for the feature ' + str(feature))\n","      return str(self.inv_map[feature][x])\n","\n","    self.statistics[feature]['n_obs'] = self.statistics[feature]['bin'].apply(bin_count)\n","    self.statistics[feature]['n_events'] = self.statistics[feature]['bin'].apply(bin_sum)\n","    self.statistics[feature]['event_rate'] = self.statistics[feature]['bin'].apply(bin_mean)\n","    self.statistics[feature]['class_list'] = self.statistics[feature]['bin'].apply(get_class_list)\n","\n","  def moveBin(self, feature, current_bin_value, next_bin_value):\n","    if feature not in self.x[self.nominal_feature_list].columns:\n","      raise NotImplementedError('Referenced feature does not exists in trained DataFrame')\n","    if feature not in self.map[feature]:\n","      raise NotImplementedError('Referenced value does not exists in feature domain')\n","    for k, v in self.map[feature].items():\n","      if current_bin_value == v:\n","        self.map[feature][k] = next_bin_value\n","\n","    self.inv_map[feature] = {}\n","    for v, b in self.map[feature].items():\n","      if b not in self.inv_map[feature]:\n","        self.inv_map[feature][b] = [v]\n","      else:\n","        self.inv_map[feature][b].append(v)\n","    \n","    self.original_inv_map[feature] = self.inv_map[feature].copy()\n","\n","    self.statistics[feature] = pd.DataFrame([k for k, v in self.inv_map[feature].items()], columns=['bin']).sort_values('bin').reset_index(drop=True)\n","\n","    def bin_count(x):\n","      b = x\n","      return self.AllData[self.AllData[feature].isin(self.inv_map[feature][b])][self.targetColumn].count()\n","\n","    def bin_mean(x):\n","      b = x\n","      return self.AllData[self.AllData[feature].isin(self.inv_map[feature][b])][self.targetColumn].mean()\n","\n","    def bin_sum(x):\n","      b = x\n","      return self.AllData[self.AllData[feature].isin(self.inv_map[feature][b])][self.targetColumn].sum()\n","    \n","    def get_class_list(x):\n","      if x not in self.inv_map[feature]:\n","        raise ValueError('bin ' + str(x) + ' does not exist for the feature ' + str(feature))\n","      return str(self.inv_map[feature][x])\n","\n","    self.statistics[feature]['n_obs'] = self.statistics[feature]['bin'].apply(bin_count)\n","    self.statistics[feature]['n_events'] = self.statistics[feature]['bin'].apply(bin_sum)\n","    self.statistics[feature]['event_rate'] = self.statistics[feature]['bin'].apply(bin_mean)\n","    self.statistics[feature]['class_list'] = self.statistics[feature]['bin'].apply(get_class_list)\n","  \n","  def fit_transform(self, x, y):\n","    self.fit(x, y)\n","    return self.transform(x)\n","  \n","  def optimalClassGrouping(self, dfFeatures, dfTarget, feature):\n","\n","    if feature not in dfFeatures.columns:\n","      raise NotImplementedError('rankColumn feature does not exists in trained DataFrame')\n","    if len(dfFeatures) != len(dfTarget):\n","      raise NotImplementedError('feature and target DataFrames must have same length')\n","\n","    targetColumn = dfTarget.columns[0]\n","\n","    series = [dfFeatures[feature], dfTarget[targetColumn].reset_index(drop=True)]\n","    dfOriginalData = pd.concat(series, axis=1)\n","\n","    if self.balanced:\n","      dfOriginalDataY1 = dfOriginalData[dfOriginalData[targetColumn] == 1]\n","      dfOriginalData = dfOriginalDataY1.append(dfOriginalData[dfOriginalData[targetColumn] == 0].sample(len(dfOriginalDataY1)), ignore_index = True)\n","    dfDerivedData = dfOriginalData\n","\n","    dfDerivedData['countColumn'] = 1\n","\n","    dfGroupRareClasses = dfDerivedData.groupby(feature).aggregate({'countColumn' : np.sum, targetColumn : np.mean}).reset_index()\n","    dfGroupRareClasses['FgRare'] = dfGroupRareClasses['countColumn'].apply(lambda x: 1 if x / sum(dfGroupRareClasses['countColumn'].values) < self.rare_cutoff_percentage else 0)\n","    dfGroupRareClasses.reset_index(level=0, inplace=True)\n","    dfGroupRareClasses['CdGroup'] = dfGroupRareClasses.apply(lambda x: x['index'] if x['FgRare'] == 0 else np.min(dfGroupRareClasses[dfGroupRareClasses['FgRare'] == 1]['index'].values), axis=1)\n","    dictCdGroup = {k : v for k, v in dfGroupRareClasses[[feature, 'CdGroup']].values}\n","\n","    dfStatistics = dfGroupRareClasses.groupby('CdGroup').aggregate({'countColumn' : np.sum, targetColumn : np.mean}).sort_values(targetColumn, ascending=False).reset_index()\n","    dfStatistics.reset_index(level=0, inplace=True)\n","    dictBin = {k:v for k, v in dfStatistics[['CdGroup', 'index']].values}\n","\n","    dfDerivedData['bin'] = dfDerivedData[feature].apply(lambda x: dictBin[dictCdGroup[x]]).astype('int64')\n","\n","    print(feature)\n","    print(dfDerivedData.groupby('bin').aggregate({'countColumn' : np.sum, targetColumn : [np.sum, np.mean]}))\n","\n","    flgFinished = False\n","    while(not flgFinished):\n","      flgUpdated = False\n","\n","      bins = dfDerivedData.bin.unique()\n","      bins.sort()\n","\n","      if len(bins) == 1:\n","        print('Process finished')\n","        return dfDerivedData\n","      \n","      bin_comparision = np.zeros((len(bins)-1, 7))\n","      for b in bins[:-1]:\n","        n_current_bin = dfDerivedData[(dfDerivedData.bin == b)].shape[0]\n","        p_current_bin = dfDerivedData[(dfDerivedData.bin == b)][targetColumn].mean()\n","\n","        n_next_bin = dfDerivedData[(dfDerivedData.bin == bins[np.where(bins == b)[0][0]+1])].shape[0]\n","        p_next_bin = dfDerivedData[(dfDerivedData.bin == bins[np.where(bins == b)[0][0]+1])][targetColumn].mean()\n","\n","        p_hat = ((n_current_bin*p_current_bin)+(n_next_bin*p_next_bin)) / (n_current_bin+n_next_bin)\n","        q_hat = 1 - p_hat\n","        std_error = ((p_hat*q_hat)*(n_current_bin+n_next_bin)/(n_current_bin*n_next_bin)) ** 0.5\n","\n","        if std_error == .0:\n","          z_value = 1\n","        else:\n","          try:\n","            z_value = (p_current_bin - p_next_bin)/std_error\n","          except:\n","            print(b, n_current_bin, p_current_bin, n_next_bin, p_next_bin, p_hat, q_hat)\n","            z_value = (p_current_bin - p_next_bin)/std_error\n","        \n","        p_value = scipy.stats.norm.sf(abs(z_value)) * 2\n","\n","        bin_comparision[np.where(bins == b)[0][0]] = [b, bins[np.where(bins == b)[0][0]+1] if p_value > self.sigLevel else b, p_value, n_current_bin, n_next_bin, p_current_bin, p_next_bin]\n","\n","        flgUpdated = True if p_value > self.sigLevel else flgUpdated\n","      \n","      #if not flgUpdated:\n","      #  print('Process finished')\n","      #  return dfDerivedData\n","\n","      if not flgUpdated and len(bins) <= self.max_bins:\n","        print('Process finished')\n","        return dfDerivedData\n","      \n","      df_bin_comparision = pd.DataFrame(bin_comparision, columns=['bin', 'next_bin' ,'p_value', 'n_current_bin', 'n_next_bin', 'p_current_bin', 'p_next_bin'])\n","\n","      #dfH0 = df_bin_comparision[df_bin_comparision.p_value > seg.sigLevel].sort_values(by='p_value', ascending=False)\n","      dfH0 = df_bin_comparision.sort_values(by='p_value', ascending=False)\n","      #dfH0_max = dfH0.ix[dfH0['p_value'].idmax()]\n","      dfH0_max = dfH0.loc[dfH0['p_value'].idxmax()]\n","\n","      current_bin = dfH0_max.bin\n","      next_bin = dfH0_max.next_bin\n","      p_value = dfH0_max.p_value\n","      n_current_bin = dfH0_max.n_current_bin\n","      n_next_bin = dfH0_max.n_next_bin\n","      p_current_bin = dfH0_max.p_current_bin\n","      p_next_bin = dfH0_max.p_next_bin\n","\n","      print('current:', current_bin, 'next_bin:', next_bin ,'p_value:', p_value, 'n_current_bin:', n_current_bin, 'n_next_bin', 'p_current_bin:', p_current_bin, 'p_next_bin:', p_next_bin)\n","\n","      print(current_bin, ' now is', next_bin)\n","      dfDerivedData.loc[dfDerivedData['bin'] == current_bin, 'bin'] = next_bin\n","      dfDerivedData['bin'] = dfDerivedData['bin'].astype('int64')\n","\n","      print(dfDerivedData.groupby('bin').aggregate({'countColumn' : np.sum, targetColumn : [np.sum, np.mean]}))\n","    \n","    return dfDerivedData.sort_values('bin').reset_index(drop=True)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMeIBzK6_8tC","executionInfo":{"status":"ok","timestamp":1605622689452,"user_tz":180,"elapsed":6584,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy\n","\n","class OptimalIntervalBinning(BaseEstimator, TransformerMixin):\n","\n","  def __init__(self, sigLevel, n_rank_groups, balanced=False, verbose=0, max_bins =10, ignore_feature_list = [], task_type = 'classification'):\n","    self.sigLevel = sigLevel\n","    self.n_rank_groups = n_rank_groups\n","    self.balanced = balanced\n","    self.verbose = verbose\n","    self.max_bins = max_bins\n","    self.ignore_feature_list = ignore_feature_list\n","    self.task_type = task_type\n","    self.ranges = {}\n","    self.original_ranges = {}\n","    self.statistics = {}\n","    self.original_statistics = {}\n","  \n","  def fit(self, x, y):\n","    self.x = x\n","    self.y = y\n","    self.targetColumn = y.columns[0]\n","    series = [x, y[self.targetColumn].reset_index(drop = True)]\n","    self.AllData = pd.concat(series, axis=1)\n","\n","    self.nominal_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] == 'category' or '_dummy_' in k or k[:5] in ('cbin_', 'ibin_')]\n","    self.nominal_feature_list = [k for k in self.nominal_feature_list if k not in self.ignore_feature_list]\n","    self.interval_feature_list = [k for k, v in x.dtypes.to_dict().items() if v.name[:len('category')] != 'category' and '_dummy_'not in k and k[:5] not in ('cbin_', 'ibin_')]\n","\n","    for feature in x[self.interval_feature_list].columns:\n","      dfBinFeature = self.optimalPercentileGrouping(x[self.interval_feature_list], y, feature)\n","      initial_statistics = dfBinFeature.groupby('bin').aggregate({feature: [np.min, np.max], 'countColumn' : np.sum, self.targetColumn : [np.sum, np.mean]})\n","      bondaries = initial_statistics.values[:,1]\n","      bins = initial_statistics.index.values\n","\n","      first_bin = bins[0]\n","      last_bin = bins[-1]\n","      \n","\n","      first_bondary = -float('inf')\n","      last_bondary = float('inf')\n","      previous_split = -float('inf')\n","      ranges = []\n","      n_range = 0\n","\n","      if first_bin == last_bin:\n","        ranges.append([n_range, first_bondary, last_bondary])\n","      else:\n","        prev_last_bin = bins[-2]\n","        for current_bin, current_split in zip(bins[:-1], bondaries[:-1]):\n","          if current_bin == first_bin:\n","            if first_bin == prev_last_bin:\n","              ranges.append([n_range, previous_split, last_bondary])\n","            else:\n","              continue\n","          else:\n","            ranges.append([n_range, previous_split, current_split])\n","            if current_bin == prev_last_bin:\n","              ranges.append([n_range+1, current_split, last_bondary])\n","          previous_split = current_split\n","          n_range += 1\n","      ranges = np.array(ranges)\n","\n","      self.ranges[feature] = ranges\n","      try:\n","        self.statistics[feature] = pd.DataFrame(ranges, columns=['n_range', 'lower_bondary', 'upper_bondary'])\n","      except Exception as ex:\n","        print('Error with feature:', feature, ranges)\n","        raise ex\n","\n","      def range_count(x):\n","        lower_bondary = x[0]\n","        upper_bondary = x[1]\n","        return self.AllData[(self.AllData[feature] >= lower_bondary) & (self.AllData[feature] < upper_bondary)][self.targetColumn].count()\n","\n","      def range_sum(x):\n","        lower_bondary = x[0]\n","        upper_bondary = x[1]\n","        return self.AllData[(self.AllData[feature] >= lower_bondary) & (self.AllData[feature] < upper_bondary)][self.targetColumn].sum()\n","\n","      def range_mean(x):\n","        lower_bondary = x[0]\n","        upper_bondary = x[1]\n","        return self.AllData[(self.AllData[feature] >= lower_bondary) & (self.AllData[feature] < upper_bondary)][self.targetColumn].mean()\n","\n","      self.statistics[feature]['n_obs'] = self.statistics[feature][['lower_bondary', 'upper_bondary']].apply(range_count, axis=1)\n","      self.statistics[feature]['n_events'] = self.statistics[feature][['lower_bondary', 'upper_bondary']].apply(range_sum, axis=1)\n","      self.statistics[feature]['event_rate'] = self.statistics[feature][['lower_bondary', 'upper_bondary']].apply(range_mean, axis=1)\n","\n","      self.original_ranges[feature] = self.ranges[feature].copy()\n","      self.original_statistics[feature] = self.statistics[feature].copy()\n","\n","  def transform(self, x):\n","    features = x\n","\n","    for feature in self.x[self.interval_feature_list].columns:\n","      if feature in self.ignore_feature_list:\n","        continue\n","      \n","      def get_ix_range(v):\n","        for new_n_range, lowerBondary, upperBondary in self.ranges[feature]:\n","          if v >= lowerBondary and v < upperBondary:\n","            return new_n_range\n","        print('feature:', feature, 'value:', v)\n","        raise NotImplementedError('Value out of the bounds')\n","      features['ibin_' + feature] = features[feature].apply(get_ix_range)\n","      features = features.drop([feature], axis=1)\n","    \n","    return features\n","  \n","  def ignore_feature(self, feature):\n","    self.ignore_feature_list.append(feature)\n","  \n","  def reset_feature(self, feature):\n","    if feature not in self.x[self.interval_feature_list].columns:\n","      raise NotImplementedError('Referenced feature does not exists in trained DataFrame')\n","    self.ranges[feature] = self.original_ranges[feature]\n","    self.statistics[feature] = self.original_statistics[feature]\n","  \n","  def splitRange(self, feature, split_range, split_value):\n","    new_ranges = []\n","    new_n_range = 0\n","\n","    if feature not in self.x[self.interval_feature_list].columns:\n","      raise NotImplementedError('Referenced feature does not exists in trained DataFrame')\n","    if split_range not in self.ranges[feature][:,0]:\n","      raise NotImplementedError('Referenced range does not exists in range list')\n","    \n","    for n_range, lowerBondary, upperBondary in self.ranges[feature]:\n","      if split_range == n_range:\n","        if not (split_value >= lowerBondary and split_value < upperBondary):\n","          raise NotImplementedError('Split value for range ' + str(split_range) + ' must be greater or equal than' + str(lowerBondary) + ' and less than ' + str(upperBondary))\n","        new_ranges.append([new_n_range, lowerBondary, split_value])\n","        new_n_range += 1\n","        new_ranges.append([new_n_range, split_value, upperBondary])\n","        new_n_range += 1\n","      else:\n","        new_ranges.append([new_n_range, lowerBondary, upperBondary])\n","        new_n_range += 1\n","\n","    new_ranges = np.array(new_ranges)\n","    self.ranges[feature] = new_ranges\n","\n","    self.statistics[feature] = pd.DataFrame(new_ranges, columns=[['n_range', 'lower_bondary', 'upper_bondary']])\n","\n","    def range_count(x):\n","      lower_bondary = x[0]\n","      upper_bondary = x[1]\n","      return self.AllData[(self.AllData[feature] >= lower_bondary) & (self.AllData[feature] < upper_bondary)][self.targetColumn].count()\n","\n","    def range_sum(x):\n","      lower_bondary = x[0]\n","      upper_bondary = x[1]\n","      return self.AllData[(self.AllData[feature] >= lower_bondary) & (self.AllData[feature] < upper_bondary)][self.targetColumn].sum()\n","\n","    def range_mean(x):\n","      lower_bondary = x[0]\n","      upper_bondary = x[1]\n","      return self.AllData[(self.AllData[feature] >= lower_bondary) & (self.AllData[feature] < upper_bondary)][self.targetColumn].mean()\n","\n","    self.statistics[feature]['n_obs'] = self.statistics[feature][['lower_bondary', 'upper_bondary']].apply(range_count, axis=1)\n","    self.statistics[feature]['n_events'] = self.statistics[feature][['lower_bondary', 'upper_bondary']].apply(range_sum, axis=1)\n","    self.statistics[feature]['event_rate'] = self.statistics[feature][['lower_bondary', 'upper_bondary']].apply(range_mean, axis=1)\n","\n","  def mergeRange(self, feature, new_start_range, new_end_range):\n","    new_ranges = []\n","    new_n_range = 0\n","    flg_start_new_range = False\n","    first_lowerBondary = -float('inf')\n","\n","    if feature not in self.x[self.interval_feature_list].columns:\n","      raise NotImplementedError('Referenced feature does not exists in trained DataFrame')\n","    if new_start_range not in self.ranges[feature][:,0]:\n","      raise NotImplementedError('Referenced start range does not exists in range list')\n","    if new_end_range not in self.ranges[feature][:,0]:\n","      raise NotImplementedError('Referenced end range does not exists in range list')\n","    \n","    for n_range, lowerBondary, upperBondary in self.ranges[feature]:\n","      if new_start_range == n_range:\n","        first_lowerBondary = lowerBondary\n","        flg_start_new_range = True\n","      elif new_end_range == n_range:\n","        flg_start_new_range = False\n","        new_ranges.append([new_n_range, first_lowerBondary, upperBondary])\n","        new_n_range += 1\n","      elif flg_start_new_range:\n","        continue\n","      else:\n","        new_ranges.append([new_n_range, lowerBondary, upperBondary])\n","        new_n_range += 1\n","    \n","    new_ranges = np.array(new_ranges)  \n","    self.ranges[feature] = new_ranges\n","\n","    self.statistics[feature] = pd.DataFrame(new_ranges, columns=[['n_range', 'lower_bondary', 'upper_bondary']])\n","\n","    def range_count(x):\n","      lower_bondary = x[0]\n","      upper_bondary = x[1]\n","      return self.AllData[(self.AllData[feature] >= lower_bondary) & (self.AllData[feature] < upper_bondary)][self.targetColumn].count()\n","\n","    def range_sum(x):\n","      lower_bondary = x[0]\n","      upper_bondary = x[1]\n","      return self.AllData[(self.AllData[feature] >= lower_bondary) & (self.AllData[feature] < upper_bondary)][self.targetColumn].sum()\n","\n","    def range_mean(x):\n","      lower_bondary = x[0]\n","      upper_bondary = x[1]\n","      return self.AllData[(self.AllData[feature] >= lower_bondary) & (self.AllData[feature] < upper_bondary)][self.targetColumn].mean()\n","\n","    self.statistics[feature]['n_obs'] = self.statistics[feature][['lower_bondary', 'upper_bondary']].apply(range_count, axis=1)\n","    self.statistics[feature]['n_events'] = self.statistics[feature][['lower_bondary', 'upper_bondary']].apply(range_sum, axis=1)\n","    self.statistics[feature]['event_rate'] = self.statistics[feature][['lower_bondary', 'upper_bondary']].apply(range_mean, axis=1)\n","  \n","  def fit_transform(self, x, y):\n","    self.fit(x, y)\n","    return self.transform(x)\n","  \n","  def optimalPercentileGrouping(self, dfFeatures, dfTarget, feature):\n","\n","    if feature not in dfFeatures.columns:\n","      raise NotImplementedError('rankColumn feature does not exists in trained DataFrame')\n","    if len(dfFeatures) != len(dfTarget):\n","      raise NotImplementedError('feature and target DataFrames must have same length')\n","\n","    rankColumn = 'rank_' + feature\n","    targetColumn = dfTarget.columns[0]\n","    featureDomain = np.unique(dfFeatures[feature].values)\n","\n","    series = [dfFeatures[feature], dfTarget[targetColumn].reset_index(drop=True)]\n","    dfOriginalData = pd.concat(series, axis=1)\n","\n","    if self.balanced:\n","      dfOriginalDataY1 = dfOriginalData[dfOriginalData[targetColumn] == 1]\n","      dfOriginalData = dfOriginalDataY1.append(dfOriginalData[dfOriginalData[targetColumn] == 0].sample(len(dfOriginalDataY1)), ignore_index = True)\n","    dfDerivedData = dfOriginalData\n","\n","    if len(featureDomain) > self.n_rank_groups:\n","      dfDerivedData[rankColumn] = (dfDerivedData[feature].rank(pct=True)*self.n_rank_groups).round().rename(rankColumn).reset_index(drop=True)\n","    else:\n","      dfDerivedData[rankColumn] = dfDerivedData[feature].rank(method='dense').rename(rankColumn).reset_index(drop=True)\n","\n","    dfDerivedData['bin'] = dfDerivedData[rankColumn].astype('int64')\n","    dfDerivedData['countColumn'] = 1\n","\n","    print(feature)\n","\n","    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n","      print(dfDerivedData.groupby('bin').aggregate({'countColumn' : np.sum, targetColumn : [np.sum, np.mean], feature : [np.min, np.max]}))\n","\n","    flgFinished = False\n","    while(not flgFinished):\n","      flgUpdated = False\n","\n","      bins = dfDerivedData.bin.unique()\n","      bins.sort()\n","\n","      if len(bins) == 1:\n","        print('Process finished')\n","        return dfDerivedData\n","      \n","      bin_comparision = np.zeros((len(bins)-1, 7))\n","      for b in bins[:-1]:\n","        n_current_bin = dfDerivedData[(dfDerivedData.bin == b)].shape[0]\n","        p_current_bin = dfDerivedData[(dfDerivedData.bin == b)][targetColumn].mean()\n","\n","        n_next_bin = dfDerivedData[(dfDerivedData.bin == bins[np.where(bins == b)[0][0]+1])].shape[0]\n","        p_next_bin = dfDerivedData[(dfDerivedData.bin == bins[np.where(bins == b)[0][0]+1])][targetColumn].mean()\n","\n","        p_hat = ((n_current_bin*p_current_bin)+(n_next_bin*p_next_bin)) / (n_current_bin+n_next_bin)\n","        q_hat = 1 - p_hat\n","        std_error = ((p_hat*q_hat)*(n_current_bin+n_next_bin)/(n_current_bin*n_next_bin)) ** 0.5\n","\n","        if std_error == .0:\n","          z_value = 1\n","        else:\n","          try:\n","            z_value = (p_current_bin - p_next_bin)/std_error\n","          except:\n","            print(b, n_current_bin, p_current_bin, n_next_bin, p_next_bin, p_hat, q_hat)\n","            z_value = (p_current_bin - p_next_bin)/std_error\n","        \n","        p_value = scipy.stats.norm.sf(abs(z_value)) * 2\n","\n","        bin_comparision[np.where(bins == b)[0][0]] = [b, bins[np.where(bins == b)[0][0]+1] if p_value > self.sigLevel or len(bins) > self.max_bins else b, p_value, n_current_bin, n_next_bin, p_current_bin, p_next_bin]\n","\n","        flgUpdated = True if p_value > self.sigLevel else flgUpdated\n","      \n","      if not flgUpdated and len(bins) <= self.max_bins:\n","        print('Process finished')\n","        return dfDerivedData\n","      \n","      df_bin_comparision = pd.DataFrame(bin_comparision, columns=['bin', 'next_bin' ,'p_value', 'n_current_bin', 'n_next_bin', 'p_current_bin', 'p_next_bin'])\n","\n","      #Adicionar lógica de número máximo de grupos no anterior\n","      dfH0 = df_bin_comparision.sort_values(by='p_value', ascending=False)\n","      #dfH0_max = dfH0.ix[dfH0['p_value'].idmax()]\n","      dfH0_max = dfH0.loc[dfH0['p_value'].idxmax()]\n","\n","      #with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n","      #  print('dfH0:', dfH0)\n","      #print('dfH0_max:', dfH0_max)\n","      \n","      #self.df_bin_comparision = df_bin_comparision\n","      #self.dfH0 = dfH0\n","      #self.dfH0_max = dfH0_max\n","\n","      current_bin = dfH0_max.bin\n","      next_bin = dfH0_max.next_bin\n","      p_value = dfH0_max.p_value\n","      n_current_bin = dfH0_max.n_current_bin\n","      n_next_bin = dfH0_max.n_next_bin\n","      p_current_bin = dfH0_max.p_current_bin\n","      p_next_bin = dfH0_max.p_next_bin\n","\n","      print('current:', current_bin, 'next_bin:', next_bin ,'p_value:', p_value, 'n_current_bin:', n_current_bin, 'n_next_bin:', n_next_bin, 'p_current_bin:', p_current_bin, 'p_next_bin:', p_next_bin)\n","\n","      print(current_bin, ' now is', next_bin)\n","      dfDerivedData.loc[dfDerivedData['bin'] == current_bin, 'bin'] = next_bin\n","      dfDerivedData['bin'] = dfDerivedData['bin'].astype('int64')\n","\n","      with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n","        print(dfDerivedData.groupby('bin').aggregate({'countColumn' : np.sum, targetColumn : [np.sum, np.mean], feature : [np.min, np.max]}))\n","    \n","    return dfDerivedData.sort_values('bin').reset_index(drop=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOt_b5Tm8qvr","executionInfo":{"status":"ok","timestamp":1605622689454,"user_tz":180,"elapsed":6581,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class ManualDropping(BaseEstimator, TransformerMixin):\n","\n","  def __ini__(self, drop_list = []):\n","    self.drop_list = drop_list\n","  \n","  def insert_new_drops(self, drop_list = []):\n","    self.drop_list += drop_list\n","  \n","  def fit(self, x, y=None):\n","    return\n","  \n","  def transform(self, x):\n","    res = x.drop([d for d in self.drop_list if d in x.columns], axis=1)\n","    if len([d for d in self.drop_list if d not in x.columns]) > 0:\n","      print('Warning: the following features weren\\'t found in current DataFrame')\n","      print([d for d in self.drop_list if d not in x.columns])\n","    return res\n","  \n","  def fit_transform(self, x):\n","    return self.transform(x)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"6i1ibu9T8rKB","executionInfo":{"status":"ok","timestamp":1605622689455,"user_tz":180,"elapsed":6579,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class ValueReplacement(BaseEstimator, TransformerMixin):\n","  \n","  def __ini__(self, from_to_rule_tuples = []):\n","    if np.max(np.array([len(t) for t in from_to_rule_tuples]) != 3):\n","      raise NotImplementedError('All tuples must have length equal to 3')\n","\n","    for feature, from_, to_ in self.from_to_rule_tuples:\n","      if not type(from_) is type(to_):\n","        raise NotImplementedError(str(from_) + ' and ' + str(to_) + ' don\\'t have same types')\n","        \n","    self.from_to_rule_tuples = from_to_rule_tuples\n","  \n","  def insert_new_rules(self, from_to_rule_tuples = []):\n","    if np.max(np.array([len(t) for t in from_to_rule_tuples]) != 3):\n","      raise NotImplementedError('All tuples must have length equal to 3')\n","\n","    for feature, from_, to_ in self.from_to_rule_tuples:\n","      if not type(from_) is type(to_):\n","        raise NotImplementedError(str(from_) + ' and ' + str(to_) + ' don\\'t have same types')\n","    \n","    self.from_to_rule_tuples += from_to_rule_tuples\n","  \n","  def fit(self, x, y=None):\n","    return\n","\n","  def transform(self, x):\n","    res = x\n","    for feature, from_, to_ in self.from_to_rule_tuples:\n","      if feature not in self.res.columns:\n","        raise NotImplementedError('Referenced feature does not exists in trained DataFrame')\n","      res.loc[res[feature] == from_, feature] = to_\n","    return res\n","\n","  def fit_transform(self, x):\n","    return self.transform(x)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"7NLoRr0q8qNX","executionInfo":{"status":"ok","timestamp":1605622689455,"user_tz":180,"elapsed":6575,"user":{"displayName":"Bruno Leme","photoUrl":"","userId":"11849490065144642495"}}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class FeatureConfiguration(BaseEstimator, TransformerMixin):\n","\n","  def __init__(self, file = ''):\n","\n","    self.data_feature_config = pd.read_csv(file, sep=';')\n","\n","    self.feature_config = {}\n","    self.feature_dtypes = {}\n","    self.key_feature_list = []\n","    self.target_feature_list = []\n","    self.drop_feature_list = []\n","    self.interval_feature_list = []\n","    self.nominal_feature_list = []\n","    self.unknown_feature_list = []\n","\n","    for index, row in self.data_feature_config.iterrows():\n","      if row['NAME'] in self.feature_config:\n","        raise ValueError('Duplicate feature configuration for ' + row['NAME'])\n","      self.feature_config[row['NAME']] = {'ROLE':row['ROLE'], 'LEVEL':row['LEVEL'], 'DROP':row['DROP']}\n","      if row['ROLE'] == 'TARGET':\n","        self.feature_dtypes[row['NAME']] = 'float64' if row['LEVEL'] in ['INTERVAL', 'BINARY'] else 'category' if row['LEVEL'] in ['NOMINAL', 'ORDINAL'] else 'object'\n","      else:\n","        self.feature_dtypes[row['NAME']] = 'float64' if row['LEVEL'] == 'INTERVAL' else 'category' if row['LEVEL'] in ['NOMINAL', 'ORDINAL', 'BINARY'] else 'object'\n","      if row['DROP'] == 'Y':\n","        self.drop_feature_list.append(row['NAME'])\n","      elif row['ROLE'] == 'TARGET':\n","        self.target_feature_list.append(row['NAME'])\n","      elif row['ROLE'] == 'ID':\n","        self.key_feature_list.append(row['NAME'])\n","      elif row['ROLE'] == 'INPUT' and row['LEVEL'] == 'INTERVAL':\n","        self.interval_feature_list.append(row['NAME'])\n","      elif row['ROLE'] == 'INPUT' and row['LEVEL'] in ['NOMINAL', 'ORDINAL', 'BINARY']:\n","        self.nominal_feature_list.append(row['NAME'])\n","      else:\n","        self.unknown_feature_list.append(row['NAME'])\n","\n","  def fit(self, x, y=None):\n","    return\n","\n","  def transform(self, x):\n","    res = x\n","    for feature, feature_dtype in self.feature_dtypes.items():\n","      if feature in self.res.columns:\n","        res[feature] = res[feature].astype(feature_dtype)\n","    res = res.drop(self.drop_feature_list, axis=1)\n","    return res\n","\n","  def fit_transform(self, x):\n","    return self.transform(x)"],"execution_count":11,"outputs":[]}]}